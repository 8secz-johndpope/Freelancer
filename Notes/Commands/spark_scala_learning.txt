1. Reduce in spark using scala :

val new_agg_df = daily_df.join(
     agg_df,
     join_seq.map(c => daily_df(c) <=> agg_df(c)).reduceLeft( _&&_ ),
     “fullouter”)
     
2. Union of 2 DF require column to be in same order, so 2 nd df is arrange as 1st one:

 agg_df.columns.union(new_df,
 .select(
       agg_df.columns.toList.head, agg_df.columns.toList.tail: _*
     ))  

3. Align to table schema       
   val schema = sqlContext.table("dse.resources_daily_cost_sum").schema
   val ff=finalDMLResourceTable.select(schema.fieldNames.map { finalDMLResourceTable(_) }:_*)
     
4. When OOM because of shuffle partitions:
spark.shuffle.io.preferDirectBufs='false'
spark.sql.shuffle.partitions = increase
spark.shuffle.registration.timeout: '600000'

5. Adaptive executions: New feature in spark, runtime decide on joining type (sort merge/broadcast hash join).
Skewness can also be removed in here.
 
