1. Reduce in spark using scala :

val new_agg_df = daily_df.join(
     agg_df,
     join_seq.map(c => daily_df(c) <=> agg_df(c)).reduceLeft( _&&_ ),
     “fullouter”)
     
2. Union of 2 DF require column to be in same order, so 2 nd df is arrange as 1st one:

 agg_df.columns.union(new_df,
 .select(
       agg_df.columns.toList.head, agg_df.columns.toList.tail: _*
     ))  

3. Align to table schema       
   val schema = sqlContext.table("table_name").schema
   val ff=finalTableDF.select(schema.fieldNames.map { finalTableDF(_) }:_*)
   
4. Increase the broadcast memory so that we can move from sort merge join to broadcast join:

 spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 1024*1024*broadCastThresholdSizeInMB)
     
5. When OOM because of shuffle partitions:
spark.shuffle.io.preferDirectBufs='false'
spark.sql.shuffle.partitions = increase
spark.shuffle.registration.timeout: '600000'

6. Adaptive executions: New feature in spark, runtime decide on joining type (sort merge/broadcast hash join).
Skewness can also be removed in here.
 
7. Add below for Adding dependency for juypter:
%AddJar s3n://netflix-dataoven-prod/genie/jars/brickhouse.jar
Internal 
%AddDep netflix-core.jar
